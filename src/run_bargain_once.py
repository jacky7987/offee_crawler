from __future__ import annotations

import argparse
import json
import re
from pathlib import Path
from typing import Iterable, List

import pandas as pd

from fetch_manifest import fetch_all_pages
from parse_product import parse_product


DEFAULT_SITEMAP = "https://www.bargain-cafe.com/sitemap.xml"
DEFAULT_BRAND = "bargain"
SKIP_KEYWORDS_DEFAULT = ("çµ„åˆ", "æ¿¾æ›", "æ¿¾ç´™", "æ¿¾æ¯", "+", "|")

TITLE_RE = re.compile(r"<title[^>]*>(.*?)</title>", re.IGNORECASE | re.DOTALL)
OG_TITLE_RE = re.compile(
    r'<meta[^>]+property=["\']og:title["\'][^>]+content=["\'](.*?)["\']',
    re.IGNORECASE | re.DOTALL,
)


def build_arg_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="æŠ“ä¸€æ¬¡ bargain sitemapï¼Œä¸‹è¼‰å•†å“é ä¸¦è§£ææˆçµæ§‹åŒ–è³‡æ–™"
    )
    parser.add_argument(
        "--sitemap-url",
        default=DEFAULT_SITEMAP,
        help="Shopline sitemap ä½ç½®ï¼ˆé è¨­ç‚º bargainï¼‰",
    )
    parser.add_argument(
        "--brand-name",
        default=DEFAULT_BRAND,
        help="å“ç‰Œåç¨±ï¼Œåƒ…åšæ—¥èªŒç”¨é€”ï¼ˆé è¨­ bargainï¼‰",
    )
    parser.add_argument(
        "--lexicon",
        type=Path,
        default=None,
        help="lexicon YAML è·¯å¾‘ï¼ˆé è¨­ data/normalize/coffee_lexicon.yamlï¼‰",
    )
    parser.add_argument(
        "--html-dir",
        type=Path,
        default=None,
        help="è‹¥ä½¿ç”¨ --use-existingï¼Œå¾æ­¤ç›®éŒ„è®€ HTMLï¼ˆé è¨­ data/raw_htmlï¼‰",
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=None,
        help="åªè™•ç†å‰ N å€‹å•†å“ï¼Œæ–¹ä¾¿é–‹ç™¼æ™‚å¿«é€Ÿé©—è­‰",
    )
    parser.add_argument(
        "--use-existing",
        action="store_true",
        help="ä¸æ‰“ APIï¼Œç›´æ¥è®€ data/raw_html è£¡é¢æ—¢æœ‰çš„æª”æ¡ˆ",
    )
    parser.add_argument(
        "--skip-keywords",
        default=",".join(SKIP_KEYWORDS_DEFAULT),
        help="è‹¥å•†å“æ¨™é¡Œå«æ­¤æ¸…å–®ä¸­çš„ä»»ä¸€é—œéµå­—å°±ç•¥éï¼Œä½¿ç”¨é€—è™Ÿåˆ†éš”ï¼ˆé è¨­ï¼šçµ„åˆ,æ¿¾æ›ï¼‰",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=None,
        help="è¼¸å‡º CSV æª”æ¡ˆè·¯å¾‘ï¼ˆé è¨­å¯«åœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„ products.csvï¼‰",
    )
    return parser


def iter_existing_html(html_dir: Path) -> Iterable[Path]:
    html_files = sorted(html_dir.glob("*.html"))
    if not html_files:
        raise SystemExit(f"âš ï¸ åœ¨ {html_dir} æ‰¾ä¸åˆ°ä»»ä½• HTMLï¼Œè«‹å…ˆç§»é™¤ --use-existing å†è·‘ä¸€æ¬¡")
    return html_files


def extract_title_from_html(html_path: Path) -> str | None:
    text = html_path.read_text(encoding="utf-8", errors="ignore")
    m = TITLE_RE.search(text)
    if m:
        return m.group(1).strip()
    m = OG_TITLE_RE.search(text)
    if m:
        return m.group(1).strip()
    return None


def should_skip_html(html_path: Path, skip_keywords: tuple[str, ...]) -> bool:
    if not skip_keywords:
        return False
    title = extract_title_from_html(html_path) or ""
    return any(keyword and keyword in title for keyword in skip_keywords)


def main() -> None:
    parser = build_arg_parser()
    args = parser.parse_args()

    skip_keywords = tuple(
        kw.strip() for kw in (args.skip_keywords or "").split(",") if kw.strip()
    )

    project_root = Path(__file__).resolve().parents[1]
    lex_yaml = args.lexicon or (project_root / "data" / "normalize" / "coffee_lexicon.yaml")
    html_dir = args.html_dir or (project_root / "data" / "raw_html")

    if args.use_existing:
        html_paths = iter_existing_html(html_dir)
    else:
        html_paths = fetch_all_pages(
            sitemap_url=args.sitemap_url,
            brand_name=args.brand_name,
            save_html=True,
        )

    if args.limit:
        html_paths = list(html_paths)[: args.limit]

    if not html_paths:
        raise SystemExit("âš ï¸ æ²’æœ‰å¯ç”¨çš„ HTML æª”æ¡ˆï¼Œè«‹ç¢ºèª sitemap æˆ–ç›®éŒ„ã€‚")

    rows: List[dict] = []
    for html_path in html_paths:
        path = Path(html_path)
        if should_skip_html(path, skip_keywords):
            print(f"â­ï¸  Skip {path.name}ï¼ˆæ¨™é¡Œå«æ’é™¤é—œéµå­—ï¼‰")
            continue

        product = parse_product(source="bargain", html_path=path, lex_yaml_path=lex_yaml)
        print(f"ğŸ“¦ Parsed {html_path}")
        print(json.dumps(product, ensure_ascii=False, indent=2))
        rows.append(product)

    if not rows:
        raise SystemExit("âš ï¸ æ²’æœ‰ä»»ä½•å•†å“è¢«è§£æï¼Œè«‹èª¿æ•´æ¢ä»¶å¾Œå†è©¦ã€‚")

    output_path = args.output or (project_root / "products.csv")
    df = pd.DataFrame(rows)
    if "norm_variety" in df.columns:
        df["norm_variety"] = df["norm_variety"].apply(
            lambda v: ", ".join(v) if isinstance(v, list) else (v or "")
        )
    df.to_csv(output_path, index=False)
    print(f"ğŸ’¾ Saved CSV to {output_path}")


if __name__ == "__main__":
    main()
